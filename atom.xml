<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L/G&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://17091557073.github.io/"/>
  <updated>2025-01-09T11:30:20.493Z</updated>
  <id>https://17091557073.github.io/</id>
  
  <author>
    <name>Gavin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>简单神经网络实现</title>
    <link href="https://17091557073.github.io/2025/01/09/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>https://17091557073.github.io/2025/01/09/神经网络代码实现/</id>
    <published>2025-01-09T09:42:00.000Z</published>
    <updated>2025-01-09T11:30:20.493Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>损失函数</em></strong></p><p>为什么不直接使用识别精度，而要引入损失函数。因为识别精度对微小的参数变化没有很好的反应（是不连续、很突然的），出于相同的原因，激活函数也不能使用阶跃函数，因为它的导数在绝大多数地方都为零，神经网络的学习将无法进行。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span><span class="params">(y, t)</span>:</span></span><br><span class="line">    <span class="comment"># 求单个数据的交叉熵误差时，需要改变数据形状</span></span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size) </span><br><span class="line">    <span class="comment"># 监督数据是one-hot-vector的情况下，转换为正确解标签的索引</span></span><br><span class="line">    <span class="keyword">if</span> t.size == y.size:</span><br><span class="line">        t = t.argmax(axis=<span class="number">1</span>)        </span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> -np.sum(np.log(y[np.arange(batch_size), t] + <span class="number">1e-7</span>)) / batch_size</span><br></pre></td></tr></table></figure></p><p><strong><em>梯度法</em></strong></p><p>神经网络在学习时使用梯度法找到最优参数（即权重和偏置，也就是损失函数取最小值时的参数），梯度表示各点处函数值减小最多的方向。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numerical_gradient</span><span class="params">(f, x)</span>:</span></span><br><span class="line">    h = <span class="number">1e-4</span> <span class="comment"># 0.0001，超参数：学习率，步长</span></span><br><span class="line">    grad = np.zeros_like(x)</span><br><span class="line">    it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line">        idx = it.multi_index</span><br><span class="line">        tmp_val = x[idx]</span><br><span class="line">        x[idx] = float(tmp_val) + h</span><br><span class="line">        fxh1 = f(x) <span class="comment"># f(x+h)</span></span><br><span class="line">        x[idx] = tmp_val - h </span><br><span class="line">        fxh2 = f(x) <span class="comment"># f(x-h)</span></span><br><span class="line">        grad[idx] = (fxh1 - fxh2) / (<span class="number">2</span>*h)＃使用中心差分来减小数值微分的误差        </span><br><span class="line">        x[idx] = tmp_val <span class="comment"># 还原值</span></span><br><span class="line">        it.iternext()   </span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure></p><p><strong><em>学习算法的实现</em></strong></p><p>以2层神经网络（隐藏层为1层）为对象，使用MNIST数据集，实现简单的手写数字识别。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> common.functions <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> common.gradient <span class="keyword">import</span> numerical_gradient</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size, weight_init_std=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">'W1'</span>] = weight_init_std * np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">'W2'</span>] = weight_init_std * np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        W1, W2 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'W2'</span>]</span><br><span class="line">        b1, b2 = self.params[<span class="string">'b1'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">    </span><br><span class="line">        a1 = np.dot(x, W1) + b1</span><br><span class="line">        z1 = sigmoid(a1)</span><br><span class="line">        a2 = np.dot(z1, W2) + b2</span><br><span class="line">        y = softmax(a2)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># x:输入数据, t:监督数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, x, t)</span>:</span></span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> cross_entropy_error(y, t)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, x, t)</span>:</span></span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        t = np.argmax(t, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        accuracy = np.sum(y == t) / float(x.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># x:输入数据, t:监督数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numerical_gradient</span><span class="params">(self, x, t)</span>:</span></span><br><span class="line">        loss_W = <span class="keyword">lambda</span> W: self.loss(x, t)</span><br><span class="line">        </span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">'W1'</span>] = numerical_gradient(loss_W, self.params[<span class="string">'W1'</span>])</span><br><span class="line">        grads[<span class="string">'b1'</span>] = numerical_gradient(loss_W, self.params[<span class="string">'b1'</span>])</span><br><span class="line">        grads[<span class="string">'W2'</span>] = numerical_gradient(loss_W, self.params[<span class="string">'W2'</span>])</span><br><span class="line">        grads[<span class="string">'b2'</span>] = numerical_gradient(loss_W, self.params[<span class="string">'b2'</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure></p><p><strong><em>效果评估</em></strong></p><p>用随机选择的小批量数据（mini-batch）作为全体训练数据的近似值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)  <span class="comment"># 为了导入父目录的文件而进行的设定</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> two_layer_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="literal">True</span>, one_hot_label=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">network = TwoLayerNet(input_size=<span class="number">784</span>, hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">iters_num = <span class="number">10000</span>  <span class="comment"># 适当设定循环的次数</span></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">train_loss_list = []</span><br><span class="line">train_acc_list = []</span><br><span class="line">test_acc_list = []</span><br><span class="line"><span class="comment">#平均每个epoch的重复次数</span></span><br><span class="line">iter_per_epoch = max(train_size / batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters_num):</span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    grad = network.numerical_gradient(x_batch, t_batch)</span><br><span class="line">    <span class="comment">#grad = network.gradient(x_batch, t_batch)#进阶高速版</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">'W1'</span>, <span class="string">'b1'</span>, <span class="string">'W2'</span>, <span class="string">'b2'</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">    </span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br><span class="line">    <span class="comment"># 计算每个epoch的识别精度，没有必要频繁记录，大致把握识别精度的推移即可</span></span><br><span class="line">    <span class="keyword">if</span> i % iter_per_epoch == <span class="number">0</span>:</span><br><span class="line">        train_acc = network.accuracy(x_train, t_train)</span><br><span class="line">        test_acc = network.accuracy(x_test, t_test)</span><br><span class="line">        train_acc_list.append(train_acc)</span><br><span class="line">        test_acc_list.append(test_acc)</span><br><span class="line">        print(<span class="string">"train acc, test acc | "</span> + str(train_acc) + <span class="string">", "</span> + str(test_acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line">markers = &#123;<span class="string">'train'</span>: <span class="string">'o'</span>, <span class="string">'test'</span>: <span class="string">'s'</span>&#125;</span><br><span class="line">x = np.arange(len(train_acc_list))</span><br><span class="line">plt.plot(x, train_acc_list, label=<span class="string">'train acc'</span>)</span><br><span class="line">plt.plot(x, test_acc_list, label=<span class="string">'test acc'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.xlabel(<span class="string">"epochs"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"accuracy"</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1.0</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/2025/01/09/神经网络代码实现/简单神经网络实现/pasted-1.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;损失函数&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;为什么不直接使用识别精度，而要引入损失函数。因为识别精度对微小的参数变化没有很好的反应（是不连续、很突然的），出于相同的原因，激活函数也不能使用阶跃函数，因为它的导数在绝大多数地方都为零，神经网络的
      
    
    </summary>
    
      <category term="神经网络" scheme="https://17091557073.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>财富自由之路</title>
    <link href="https://17091557073.github.io/2024/12/25/%E8%B4%A2%E5%AF%8C%E8%87%AA%E7%94%B1%E4%B9%8B%E8%B7%AF/"/>
    <id>https://17091557073.github.io/2024/12/25/财富自由之路/</id>
    <published>2024-12-25T02:23:00.000Z</published>
    <updated>2025-01-06T09:42:23.220Z</updated>
    
    <content type="html"><![CDATA[<p>财富只是工具，自由才是最终的目的，自由的本质是时间的自主权。<br><strong><em>财富自由</em></strong>是指不需要为了满足生活必需而出售自己的时间了。</p><p>专注<strong><em>成长</em></strong>而不是专注成功，财富自由不是终点，只是通往过程中的一个里程碑。<br>每一次选择从积累能力的角度出发，要考虑自己还需要什么能力。</p><p>重视<strong><em>价值</em></strong>忽略估值，长期来看，估值是虚幻的，虽有波动，却实际锚定在价值上的，切勿自欺欺人。</p><p><strong><em>耐心</em></strong>有复利效应，不能做自己喜欢的事情是人生常态，困难的阶段不过是必经之路。耐心是一切成长的刚需，几乎所有的半途而废，最终都可以归结到这一点：短期期望过高。</p><p>不要凑热闹、随大流、瞎操心，把最宝贵的<strong><em>注意力</em></strong>，放在你的成长、真爱和对社会有贡献的事情上。</p><p>百分百的<strong><em>安全</em></strong>带来的是百分百的束缚，几乎所有的进步都是放弃了部分安全感获得的。</p><p><strong><em>元认知能力</em></strong>：我正在想的是什么；我这么想对不对；我应该怎么想才对。</p><p>不能改变的东西尝试接受它；可以改变的东西努力解决它；<strong><em>抱怨</em></strong>是无奈和无能的表现。</p><p><strong><em>有勇</em></strong>：愿意为自己的行为负责，不论做什么事，都不推卸责任，承担一切后果；<strong><em>有谋</em></strong>：不会意气用事，做事讲逻辑，尊重事物发展的规律，不违背大势行动。</p><p>人所拥有的任何东西都可以被剥夺，唯独<strong><em>人性最后的自由</em></strong>（也就是在任何境遇中选择自己态度和生活方式的自由）不能被剥夺。</p><p><strong><em>认清差异</em></strong>是我们消除差异的第一步。不能一味地否定事实，要敢于正视现实差距，看到别人的财富，要想到这只是内在差距的外在体现而已。试着去学习对方的优点，从内到外不断磨平差距。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;财富只是工具，自由才是最终的目的，自由的本质是时间的自主权。&lt;br&gt;&lt;strong&gt;&lt;em&gt;财富自由&lt;/em&gt;&lt;/strong&gt;是指不需要为了满足生活必需而出售自己的时间了。&lt;/p&gt;
&lt;p&gt;专注&lt;strong&gt;&lt;em&gt;成长&lt;/em&gt;&lt;/strong&gt;而不是专注成功，财富自由
      
    
    </summary>
    
      <category term="Daily life" scheme="https://17091557073.github.io/categories/Daily-life/"/>
    
    
  </entry>
  
  <entry>
    <title>激活函数</title>
    <link href="https://17091557073.github.io/2024/12/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://17091557073.github.io/2024/12/24/神经网络/</id>
    <published>2024-12-24T13:16:00.000Z</published>
    <updated>2024-12-24T16:18:46.619Z</updated>
    
    <content type="html"><![CDATA[<h4 id="线性"><a href="#线性" class="headerlink" title="线性"></a>线性</h4><p>h(x)=cx</p><p>神经网络的激活函数一般会使用非线性函数，若使用线性函数则加深层数就没有意义了，如h(h(h(x)))=c^3*x</p><h4 id="非线性"><a href="#非线性" class="headerlink" title="非线性"></a>非线性</h4><p>最后输出层所用的激活函数，要根据求解问题的性质决定。一般地，回归问题用恒等函数；二元分类问题用sigmoid函数；多元分类问题用softmax函数。</p><p>1、阶跃函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_function</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.array(x&gt;<span class="number">0</span>,dtype=np.int)</span><br></pre></td></tr></table></figure></p><p>2、sigmoid函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br></pre></td></tr></table></figure></p><p>3、ReLU函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br></pre></td></tr></table></figure></p><p>4、softmax函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x.ndim == <span class="number">2</span>:</span><br><span class="line">        x = x.T</span><br><span class="line">        x = x - np.max(x, axis=<span class="number">0</span>)</span><br><span class="line">        y = np.exp(x) / np.sum(np.exp(x), axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> y.T </span><br><span class="line">    x = x - np.max(x) <span class="comment"># 溢出对策</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / np.sum(np.exp(x))</span><br></pre></td></tr></table></figure></p><h4 id="案例：手写数字识别"><a href="#案例：手写数字识别" class="headerlink" title="案例：手写数字识别"></a>案例：手写数字识别</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys,os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line">(x_train,t_train),(x_test,t_test)=load_mnist(flatten=<span class="literal">True</span>,normalize=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> (x_train,t_train,x_test,t_test):</span><br><span class="line">    print(i.shape)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_show</span><span class="params">(img)</span>:</span></span><br><span class="line">    pil_img=Image.fromarray(np.uint8(img))</span><br><span class="line">    pil_img.show()</span><br><span class="line">img=x_train[<span class="number">0</span>].reshape(<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">label=t_train[<span class="number">0</span>]</span><br><span class="line">img_show(img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">()</span>:</span></span><br><span class="line">    (x_train,t_train),(x_test,t_test)=load_mnist(normalize=<span class="literal">True</span>,flatten=<span class="literal">True</span>,one_hot_label=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> x_test,t_test</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_network</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''假设之前学习已经完成，将学习到的参数保存下来，现在直接加载'''</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"sample_weight.pkl"</span>,<span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        network=pickle.load(f)</span><br><span class="line">        <span class="keyword">return</span> network</span><br><span class="line"><span class="keyword">from</span> common.functions <span class="keyword">import</span> sigmoid, softmax</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(network,x)</span>:</span></span><br><span class="line">    W1,W2,W3=network[<span class="string">'W1'</span>],network[<span class="string">'W2'</span>],network[<span class="string">'W3'</span>]    </span><br><span class="line">    b1,b2,b3=network[<span class="string">'b1'</span>],network[<span class="string">'b2'</span>],network[<span class="string">'b3'</span>]</span><br><span class="line">    a1=np.dot(x,W1)+b1</span><br><span class="line">    z1=sigmoid(a1)</span><br><span class="line">    a2=np.dot(z1,W2)+b2</span><br><span class="line">    z2=sigmoid(a2)</span><br><span class="line">    a3 = np.dot(z2, W3) + b3</span><br><span class="line">    y = softmax(a3)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">x,t=get_data()</span><br><span class="line">network=init_network()</span><br><span class="line">accuracy_cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">    y = predict(network, x[i])</span><br><span class="line">    p= np.argmax(y) <span class="comment"># 获取概率最高的元素的索引</span></span><br><span class="line">    <span class="keyword">if</span> p == t[i]:</span><br><span class="line">        accuracy_cnt += <span class="number">1</span></span><br><span class="line">print(<span class="string">"Accuracy:"</span> + str(float(accuracy_cnt) / len(x)))</span><br><span class="line"><span class="comment">#优化流程，修改为批处理方式，大幅提高计算速度</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">accuracy_cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(x), batch_size):</span><br><span class="line">    x_batch = x[i:i+batch_size]</span><br><span class="line">    y_batch = predict(network, x_batch)</span><br><span class="line">    p = np.argmax(y_batch, axis=<span class="number">1</span>)</span><br><span class="line">    accuracy_cnt += np.sum(p == t[i:i+batch_size])</span><br><span class="line">print(<span class="string">"Accuracy:"</span> + str(float(accuracy_cnt) / len(x)))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;线性&quot;&gt;&lt;a href=&quot;#线性&quot; class=&quot;headerlink&quot; title=&quot;线性&quot;&gt;&lt;/a&gt;线性&lt;/h4&gt;&lt;p&gt;h(x)=cx&lt;/p&gt;
&lt;p&gt;神经网络的激活函数一般会使用非线性函数，若使用线性函数则加深层数就没有意义了，如h(h(h(x)))=c^3*
      
    
    </summary>
    
      <category term="神经网络" scheme="https://17091557073.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>感知机</title>
    <link href="https://17091557073.github.io/2024/12/23/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>https://17091557073.github.io/2024/12/23/感知机/</id>
    <published>2024-12-23T11:56:00.000Z</published>
    <updated>2024-12-24T13:10:21.957Z</updated>
    
    <content type="html"><![CDATA[<h6 id="单层感知机"><a href="#单层感知机" class="headerlink" title="单层感知机"></a>单层感知机</h6><p>又称朴素感知机，只能表示线性空间，以下示例参数（w1,w2,b）由人工确定。</p><ul><li><p>与门<br><img src="/2024/12/23/感知机/pasted-1.png" alt="upload successful"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AND</span><span class="params">(x1,x2)</span>:</span></span><br><span class="line">    <span class="comment">#输入信号</span></span><br><span class="line">    x=np.array([x1,x2])</span><br><span class="line">    <span class="comment">#权重，控制输入信号的重要性</span></span><br><span class="line">    w=np.array([<span class="number">0.5</span>,<span class="number">0.5</span>])</span><br><span class="line">    <span class="comment">#偏置，调整神经元被激活的容易程度</span></span><br><span class="line">    b=<span class="number">-0.7</span></span><br><span class="line">    <span class="comment">#输出</span></span><br><span class="line">    tmp=np.sum(w*x)+b</span><br><span class="line">    <span class="keyword">if</span> tmp&lt;=<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>或门<br><img src="/2024/12/23/感知机/pasted-3.png" alt="upload successful"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OR</span><span class="params">(x1,x2)</span>:</span></span><br><span class="line">    x=np.array([x1,x2])</span><br><span class="line">    <span class="comment">#仅权重和偏置与AND不同</span></span><br><span class="line">    w=np.array([<span class="number">0.5</span>,<span class="number">0.5</span>])</span><br><span class="line">    b=<span class="number">0.2</span></span><br><span class="line">    tmp=np.sum(w*x)+b</span><br><span class="line">    <span class="keyword">if</span> tmp&lt;=<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>与非门<br><img src="/2024/12/23/感知机/pasted-2.png" alt="upload successful"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">NAND</span><span class="params">(x1,x2)</span>:</span></span><br><span class="line">    x=np.array([x1,x2])</span><br><span class="line">    <span class="comment">#仅权重和偏置与AND不同</span></span><br><span class="line">    w=np.array([<span class="number">-0.5</span>,<span class="number">-0.5</span>])</span><br><span class="line">    b=<span class="number">0.7</span></span><br><span class="line">    tmp=np.sum(w*x)+b</span><br><span class="line">    <span class="keyword">if</span> tmp&lt;=<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul><h6 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h6><p>可以表示非线性空间</p><p><img src="/2024/12/23/感知机/pasted-5.png" alt="upload successful"></p><ul><li>异或门<br><img src="/2024/12/23/感知机/pasted-4.png" alt="upload successful"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">XOR</span><span class="params">(x1,x2)</span>:</span></span><br><span class="line">    s1=NAND(x1,x2)</span><br><span class="line">    s2=OR(x1,x2)</span><br><span class="line">    y=AND(s1,s2)</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h6 id=&quot;单层感知机&quot;&gt;&lt;a href=&quot;#单层感知机&quot; class=&quot;headerlink&quot; title=&quot;单层感知机&quot;&gt;&lt;/a&gt;单层感知机&lt;/h6&gt;&lt;p&gt;又称朴素感知机，只能表示线性空间，以下示例参数（w1,w2,b）由人工确定。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;与门&lt;
      
    
    </summary>
    
      <category term="神经网络" scheme="https://17091557073.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>等额本金 or 本息区别</title>
    <link href="https://17091557073.github.io/2021/10/06/%E7%AD%89%E9%A2%9D%E6%9C%AC%E9%87%91vs%E7%AD%89%E9%A2%9D%E6%9C%AC%E6%81%AF/"/>
    <id>https://17091557073.github.io/2021/10/06/等额本金vs等额本息/</id>
    <published>2021-10-06T02:22:00.000Z</published>
    <updated>2025-01-06T12:04:12.275Z</updated>
    
    <content type="html"><![CDATA[<h6 id="月还款额及利息公式推导"><a href="#月还款额及利息公式推导" class="headerlink" title="月还款额及利息公式推导"></a>月还款额及利息公式推导</h6><p><img src="/2021/10/06/等额本金vs等额本息/公式推导.jpg" alt="公式推导"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h6 id=&quot;月还款额及利息公式推导&quot;&gt;&lt;a href=&quot;#月还款额及利息公式推导&quot; class=&quot;headerlink&quot; title=&quot;月还款额及利息公式推导&quot;&gt;&lt;/a&gt;月还款额及利息公式推导&lt;/h6&gt;&lt;p&gt;&lt;img src=&quot;/2021/10/06/等额本金vs等额本息/公
      
    
    </summary>
    
      <category term="金融" scheme="https://17091557073.github.io/categories/%E9%87%91%E8%9E%8D/"/>
    
    
      <category term="等额本息" scheme="https://17091557073.github.io/tags/%E7%AD%89%E9%A2%9D%E6%9C%AC%E6%81%AF/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机</title>
    <link href="https://17091557073.github.io/2019/06/08/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://17091557073.github.io/2019/06/08/支持向量机/</id>
    <published>2019-06-08T08:43:00.000Z</published>
    <updated>2019-06-16T16:40:20.781Z</updated>
    
    <content type="html"><![CDATA[<h6 id="基本线性SVM推导"><a href="#基本线性SVM推导" class="headerlink" title="基本线性SVM推导"></a>基本线性SVM推导</h6><p><a href="https://mp.weixin.qq.com/s/Ahvp0IAdgK9OVHFXigBk_Q" target="_blank" rel="noopener"><em>参考文献_线性SVM</em></a><br><a href="https://mp.weixin.qq.com/s/Q5bFR3vDDXPhtzXlVAE3Rg" target="_blank" rel="noopener"><em>参考文献_对偶SVM</em></a><br><img src="/2019/06/08/支持向量机/基本线性SVM推导.jpg" alt="基本线性SVM推导"></p><h6 id="SMO优化算法推导"><a href="#SMO优化算法推导" class="headerlink" title="SMO优化算法推导"></a>SMO优化算法推导</h6><p>在KKT约束条件下,最小化目标函数。序列最小优化（SMO）：<br><img src="/2019/06/08/支持向量机/SMO推导.jpg" alt="SMO推导"></p><h6 id="超参数详解"><a href="#超参数详解" class="headerlink" title="超参数详解"></a>超参数详解</h6><p><img src="/2019/06/08/支持向量机/超参数详解.png" alt="超参数详解"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h6 id=&quot;基本线性SVM推导&quot;&gt;&lt;a href=&quot;#基本线性SVM推导&quot; class=&quot;headerlink&quot; title=&quot;基本线性SVM推导&quot;&gt;&lt;/a&gt;基本线性SVM推导&lt;/h6&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/Ahvp0I
      
    
    </summary>
    
      <category term="机器学习实战" scheme="https://17091557073.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="SMO" scheme="https://17091557073.github.io/tags/SMO/"/>
    
      <category term="KKT条件" scheme="https://17091557073.github.io/tags/KKT%E6%9D%A1%E4%BB%B6/"/>
    
      <category term="核函数" scheme="https://17091557073.github.io/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>Logistic回归</title>
    <link href="https://17091557073.github.io/2019/05/26/Logistic%E5%9B%9E%E5%BD%92/"/>
    <id>https://17091557073.github.io/2019/05/26/Logistic回归/</id>
    <published>2019-05-26T15:00:00.000Z</published>
    <updated>2019-06-10T15:30:52.681Z</updated>
    
    <content type="html"><![CDATA[<h6 id="主要思想："><a href="#主要思想：" class="headerlink" title="主要思想："></a><strong>主要思想：</strong></h6><p>根据现有数据，对分类边界线建立回归方程（用一条直线对数据点进行拟合的过程称作回归），使用最优化算法，找到最佳拟合参数集。 </p><h6 id="Sigmoid函数："><a href="#Sigmoid函数：" class="headerlink" title="Sigmoid函数："></a><strong>Sigmoid函数：</strong></h6><p>为了实现Logistic回归分类器，我们在每个特征上都乘以一个回归系数，然后把所有结果值相加，将这个总和代入Sigmoid阶跃函数中，进而得到一个[0,1]之间的数值，大于0.5的数据归入1类，小于0.5归入0类，所以Logistic回归也被看成一个概率估计。</p><h6 id="极大似然估计-amp-交叉熵损失函数："><a href="#极大似然估计-amp-交叉熵损失函数：" class="headerlink" title="极大似然估计&amp;交叉熵损失函数："></a><strong>极大似然估计&amp;交叉熵损失函数：</strong></h6><p><img src="/2019/05/26/Logistic回归/交叉熵损失函数.jpg" alt="交叉熵损失函数推导"></p><h6 id="最优化算法："><a href="#最优化算法：" class="headerlink" title="最优化算法："></a><strong>最优化算法：</strong></h6><ul><li>梯度下降求损失函数最小值<br>  <img src="/2019/05/26/Logistic回归/交叉熵损失函数求偏导.png" alt="交叉熵损失函数求偏导"></li><li>梯度上升算法<br>要找到某个函数的最大值，最好的方法就是沿着该函数的梯度方向（即函数值增长最快的方向）探寻，设置步长，每移动一步到达下个点后会重新计算梯度方向，如此循环迭代，直至迭代次数达到某个指定值或算法达到某个可以允许的误差范围。</li><li>随机梯度上升算法<br>  对梯度上升算法的改进，每次仅用一个样本点来更新回归系数，降低算法复杂度。可以在新样本到来时，对分类器进行增量式参数更新，不需要重新读取整个数据集进行批处理运算，是一种<strong>在线学习</strong>算法。</li></ul><hr><h6 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Sun Jun  2 17:08:04 2019</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: LiGuan</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@desc: 逻辑回归</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载数据集</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    dataMat,labelMat=[],[]</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">r'C:\Users\ligua\Videos\我的资源\机器学习实战\machinelearninginaction\Ch05\testSet.txt'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">            lineArr=line.strip().split()</span><br><span class="line">            <span class="comment">#设置初始截距项</span></span><br><span class="line">            dataMat.append([<span class="number">1</span>,float(lineArr[<span class="number">0</span>]),float(lineArr[<span class="number">1</span>])])</span><br><span class="line">            labelMat.append(int(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(inX)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    阶跃函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-inX))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn,classLabels)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    最优化算法：梯度上升</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    dataMatrix=np.mat(dataMatIn)</span><br><span class="line">    labelMat=np.mat(classLabels).transpose()</span><br><span class="line">    m,n=np.shape(dataMatrix)</span><br><span class="line">    <span class="comment">#定义步长、最大迭代次数和初始权重参数</span></span><br><span class="line">    alpha=<span class="number">0.001</span></span><br><span class="line">    maxCycles=<span class="number">500</span></span><br><span class="line">    weights=np.ones((n,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</span><br><span class="line">        h=sigmoid(dataMatrix*weights)</span><br><span class="line">        error=labelMat-h</span><br><span class="line">        weights=weights+alpha*dataMatrix.transpose()*error</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatrix,classLabels)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    随机梯度上升算法</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m,n=np.shape(dataMatrix)</span><br><span class="line">    alpha=<span class="number">0.01</span></span><br><span class="line">    weights=np.ones(n)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        h=sigmoid(sum(dataMatrix[i]*weights))</span><br><span class="line">        error=classLabels[i]-h</span><br><span class="line">        weights=weights+alpha*error*dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatrix,classLabels,numIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    改进后的随机梯度算法</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m,n=np.shape(dataMatrix)</span><br><span class="line">    weights=np.ones(n)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        dataIndex=list(range(m))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            alpha=<span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.01</span><span class="comment">#在每次迭代的时候调整步长,加一个常数项，永远不会减小到0，保证多次迭代后新数据仍然具有一定的影响</span></span><br><span class="line">            randIndex=int(random.uniform(<span class="number">0</span>,len(dataIndex)))<span class="comment">#随机选取样本更新系数，减少周期性波动</span></span><br><span class="line">            h=sigmoid(sum(dataMatrix[randIndex]*weights))</span><br><span class="line">            error=classLabels[randIndex]-h</span><br><span class="line">            weights=weights+alpha*error*dataMatrix[randIndex]</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"> </span><br><span class="line">    </span><br><span class="line"><span class="comment">#应用实例：从疝气病症预测病马的死亡率</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">缺失值处理：</span></span><br><span class="line"><span class="string">    1、使用可用特征的均值来填补缺失值；</span></span><br><span class="line"><span class="string">    2、使用特殊值来填补缺失值，如-1；</span></span><br><span class="line"><span class="string">    3、忽略有缺失值的样本；</span></span><br><span class="line"><span class="string">    4、使用相似样本的均值填补缺失值；</span></span><br><span class="line"><span class="string">    5、使用另外的机器学习算法预测缺失值。</span></span><br><span class="line"><span class="string">    6、标签如果缺失，很难采取某个合适的值进行替换，建议直接删除该样本</span></span><br><span class="line"><span class="string">    我们选用0来替换所有的缺失值，更新的时候不会影响系数的值；该数据集中特征取值一般不为0，所以也满足了特殊值的要求</span></span><br><span class="line"><span class="string">    类别标签：仍存活、已经死亡、已经安乐死，将后两种合并成未能存活，改成一个二分类问题</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span><span class="params">(inX,weight)</span>:</span></span><br><span class="line">    prob=sigmoid(sum(inX*weight))</span><br><span class="line">    <span class="keyword">if</span> prob&gt;<span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colicTest</span><span class="params">()</span>:</span></span><br><span class="line">    frTrain=open(<span class="string">r'C:\Users\ligua\Videos\我的资源\机器学习实战\machinelearninginaction\Ch05\horseColicTraining.txt'</span>)</span><br><span class="line">    frTest=open(<span class="string">r'C:\Users\ligua\Videos\我的资源\机器学习实战\machinelearninginaction\Ch05\horseColicTest.txt'</span>)</span><br><span class="line">    trainingSet,trainingLabels=[],[]</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTrain.readlines():</span><br><span class="line">        currLine=line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        trainingSet.append(lineArr)</span><br><span class="line">        trainingLabels.append(float(currLine[<span class="number">21</span>]))</span><br><span class="line">    trainWeights=stocGradAscent1(np.array(trainingSet),trainingLabels,<span class="number">500</span>)</span><br><span class="line">    errorCount,numTestVec=<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTest.readlines():</span><br><span class="line">        numTestVec+=<span class="number">1.0</span></span><br><span class="line">        currLine=line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        <span class="keyword">if</span> int(classifyVector(np.array(lineArr),trainWeights))!=int(currLine[<span class="number">21</span>]):</span><br><span class="line">            errorCount+=<span class="number">1</span></span><br><span class="line">    frTrain.close()</span><br><span class="line">    frTest.close()</span><br><span class="line">    errorRate=errorCount/numTestVec</span><br><span class="line">    print(<span class="string">'the error rate of this test is:%f'</span>%(errorRate))</span><br><span class="line">    <span class="keyword">return</span> errorRate</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mulitTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算十次结果取平均值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    numTests,errorSum=<span class="number">10</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(numTests):</span><br><span class="line">        errorSum+=colicTest()</span><br><span class="line">    print(<span class="string">'after %d iterations the average error rate is:%f'</span>%(numTests,errorSum/float(numTests)))</span><br></pre></td></tr></table></figure><h6 id="模块调用"><a href="#模块调用" class="headerlink" title="模块调用"></a>模块调用</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logRegres</span><br><span class="line">dataArr,labelMat=logRegres.loadDataSet()</span><br><span class="line">logRegres.gradAscent(dataArr,labelMat)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">imp.reload(logRegres)</span><br><span class="line">dataArr,labelMat=logRegres.loadDataSet()</span><br><span class="line">logRegres.stocGradAscent0(np.array(dataArr),labelMat)</span><br><span class="line"></span><br><span class="line">imp.reload(logRegres)</span><br><span class="line">dataArr,labelMat=logRegres.loadDataSet()</span><br><span class="line">logRegres.stocGradAscent1(np.array(dataArr),labelMat)</span><br><span class="line"></span><br><span class="line">imp.reload(logRegres)</span><br><span class="line">logRegres.mulitTest()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h6 id=&quot;主要思想：&quot;&gt;&lt;a href=&quot;#主要思想：&quot; class=&quot;headerlink&quot; title=&quot;主要思想：&quot;&gt;&lt;/a&gt;&lt;strong&gt;主要思想：&lt;/strong&gt;&lt;/h6&gt;&lt;p&gt;根据现有数据，对分类边界线建立回归方程（用一条直线对数据点进行拟合的过程称作回归）
      
    
    </summary>
    
      <category term="机器学习实战" scheme="https://17091557073.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="交叉熵" scheme="https://17091557073.github.io/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/"/>
    
      <category term="梯度下降法" scheme="https://17091557073.github.io/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
    
      <category term="极大似然估计" scheme="https://17091557073.github.io/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>O2O优惠券使用预测</title>
    <link href="https://17091557073.github.io/2019/05/10/o2o-1/"/>
    <id>https://17091557073.github.io/2019/05/10/o2o-1/</id>
    <published>2019-05-10T08:19:00.000Z</published>
    <updated>2024-12-31T15:55:34.944Z</updated>
    
    <content type="html"><![CDATA[<p><strong>赛题链接：</strong></p><p><a href="https://tianchi.aliyun.com/getStart/introduction.htm?spm=5176.100066.0.0.518433afBqXIKM&amp;raceId=231593" target="_blank" rel="noopener">天池o2o优惠券使用预测</a></p><h2 id="导入相关库"><a href="#导入相关库" class="headerlink" title="导入相关库"></a>导入相关库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import libraries necessary for this project</span></span><br><span class="line"><span class="keyword">import</span> os, sys, pickle</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> date</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier, LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> log_loss, roc_auc_score, auc, roc_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"> </span><br><span class="line"><span class="comment"># display for this notebook</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br></pre></td></tr></table></figure><h2 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dfoff = pd.read_csv(<span class="string">'data/ccf_offline_stage1_train.csv'</span>)</span><br><span class="line">dfon = pd.read_csv(<span class="string">'data/ccf_online_stage1_train.csv'</span>)</span><br><span class="line">dftest = pd.read_csv(<span class="string">'data/ccf_offline_stage1_test_revised.csv'</span>)</span><br><span class="line"> </span><br><span class="line">dfoff.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="简单统计"><a href="#简单统计" class="headerlink" title="简单统计"></a>简单统计</h3><p>简单统计一下用户使用优惠券的情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'有优惠卷，购买商品：%d'</span> % dfoff[(dfoff[<span class="string">'Date_received'</span>] != <span class="string">'null'</span>) &amp; (dfoff[<span class="string">'Date'</span>] != <span class="string">'null'</span>)].shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'有优惠卷，未购商品：%d'</span> % dfoff[(dfoff[<span class="string">'Date_received'</span>] != <span class="string">'null'</span>) &amp; (dfoff[<span class="string">'Date'</span>] == <span class="string">'null'</span>)].shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'无优惠卷，购买商品：%d'</span> % dfoff[(dfoff[<span class="string">'Date_received'</span>] == <span class="string">'null'</span>) &amp; (dfoff[<span class="string">'Date'</span>] != <span class="string">'null'</span>)].shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'无优惠卷，未购商品：%d'</span> % dfoff[(dfoff[<span class="string">'Date_received'</span>] == <span class="string">'null'</span>) &amp; (dfoff[<span class="string">'Date'</span>] == <span class="string">'null'</span>)].shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>有优惠卷，购买商品：75382有优惠卷，未购商品：977900无优惠卷，购买商品：701602无优惠卷，未购商品：0</code></pre><p>可见，很多人（701602）购买商品却没有使用优惠券，也有很多人（977900）有优惠券但却没有使用，真正使用优惠券购买商品的人（75382）很少！所以，这个比赛的意义就是把优惠券送给真正可能会购买商品的人。</p><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfoff.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="打折率-Discount-rate"><a href="#打折率-Discount-rate" class="headerlink" title="打折率 Discount_rate"></a>打折率 Discount_rate</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Discount_rate 类型：\n'</span>,dfoff[<span class="string">'Discount_rate'</span>].unique())</span><br></pre></td></tr></table></figure><pre><code>Discount_rate 类型： [&#39;null&#39; &#39;150:20&#39; &#39;20:1&#39; &#39;200:20&#39; &#39;30:5&#39; &#39;50:10&#39; &#39;10:5&#39; &#39;100:10&#39; &#39;200:30&#39; &#39;20:5&#39; &#39;30:10&#39; &#39;50:5&#39; &#39;150:10&#39; &#39;100:30&#39; &#39;200:50&#39; &#39;100:50&#39; &#39;300:30&#39; &#39;50:20&#39; &#39;0.9&#39; &#39;10:1&#39; &#39;30:1&#39; &#39;0.95&#39; &#39;100:5&#39; &#39;5:1&#39; &#39;100:20&#39; &#39;0.8&#39; &#39;50:1&#39; &#39;200:10&#39; &#39;300:20&#39; &#39;100:1&#39; &#39;150:30&#39; &#39;300:50&#39; &#39;20:10&#39; &#39;0.85&#39; &#39;0.6&#39; &#39;150:50&#39; &#39;0.75&#39; &#39;0.5&#39; &#39;200:5&#39; &#39;0.7&#39; &#39;30:20&#39; &#39;300:10&#39; &#39;0.2&#39; &#39;50:30&#39; &#39;200:100&#39; &#39;150:5&#39;]</code></pre><p>打折率分为 3 种情况：</p><ul><li><p>‘null’ 表示没有打折</p></li><li><p>[0,1] 表示折扣率</p></li><li><p>x:y 表示满x减y</p></li></ul><p><strong>处理方式：</strong></p><ul><li><p>打折类型：getDiscountType()</p></li><li><p>折扣率：convertRate()</p></li><li><p>满多少：getDiscountMan()</p></li><li><p>减多少：getDiscountJian()</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert Discount_rate and Distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDiscountType</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> row == <span class="string">'null'</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'null'</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="string">':'</span> <span class="keyword">in</span> row:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convertRate</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="string">"""Convert discount to rate"""</span></span><br><span class="line">    <span class="keyword">if</span> row == <span class="string">'null'</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="string">':'</span> <span class="keyword">in</span> row:</span><br><span class="line">        rows = row.split(<span class="string">':'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span> - float(rows[<span class="number">1</span>])/float(rows[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> float(row)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDiscountMan</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">':'</span> <span class="keyword">in</span> row:</span><br><span class="line">        rows = row.split(<span class="string">':'</span>)</span><br><span class="line">        <span class="keyword">return</span> int(rows[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDiscountJian</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">':'</span> <span class="keyword">in</span> row:</span><br><span class="line">        rows = row.split(<span class="string">':'</span>)</span><br><span class="line">        <span class="keyword">return</span> int(rows[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processData</span><span class="params">(df)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert discount_rate</span></span><br><span class="line">    df[<span class="string">'discount_type'</span>] = df[<span class="string">'Discount_rate'</span>].apply(getDiscountType)</span><br><span class="line">    df[<span class="string">'discount_rate'</span>] = df[<span class="string">'Discount_rate'</span>].apply(convertRate)</span><br><span class="line">    df[<span class="string">'discount_man'</span>] = df[<span class="string">'Discount_rate'</span>].apply(getDiscountMan)</span><br><span class="line">    df[<span class="string">'discount_jian'</span>] = df[<span class="string">'Discount_rate'</span>].apply(getDiscountJian)</span><br><span class="line">    </span><br><span class="line">    print(df[<span class="string">'discount_rate'</span>].unique())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfoff = processData(dfoff)</span><br><span class="line">dftest = processData(dftest)</span><br></pre></td></tr></table></figure><pre><code>[ 1.          0.86666667  0.95        0.9         0.83333333  0.8         0.5  0.85        0.75        0.66666667  0.93333333  0.7         0.6  0.96666667  0.98        0.99        0.975       0.33333333  0.2         0.4       ][ 0.83333333  0.9         0.96666667  0.8         0.95        0.75        0.98  0.5         0.86666667  0.6         0.66666667  0.7         0.85  0.33333333  0.94        0.93333333  0.975       0.99      ]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfoff.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="距离-Distance"><a href="#距离-Distance" class="headerlink" title="距离 Distance"></a>距离 Distance</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Distance 类型：'</span>,dfoff[<span class="string">'Distance'</span>].unique())</span><br></pre></td></tr></table></figure><pre><code>Distance 类型： [&#39;0&#39; &#39;1&#39; &#39;null&#39; &#39;2&#39; &#39;10&#39; &#39;4&#39; &#39;7&#39; &#39;9&#39; &#39;3&#39; &#39;5&#39; &#39;6&#39; &#39;8&#39;]</code></pre><p>将距离 str 转为 int。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># convert distance</span></span><br><span class="line">dfoff[<span class="string">'distance'</span>] = dfoff[<span class="string">'Distance'</span>].replace(<span class="string">'null'</span>, <span class="number">-1</span>).astype(int)</span><br><span class="line">print(dfoff[<span class="string">'distance'</span>].unique())</span><br><span class="line">dftest[<span class="string">'distance'</span>] = dftest[<span class="string">'Distance'</span>].replace(<span class="string">'null'</span>, <span class="number">-1</span>).astype(int)</span><br><span class="line">print(dftest[<span class="string">'distance'</span>].unique())</span><br></pre></td></tr></table></figure><pre><code>[ 0  1 -1  2 10  4  7  9  3  5  6  8][ 1 -1  5  2  0 10  3  6  7  4  9  8]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfoff.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="领劵日期-Date-received"><a href="#领劵日期-Date-received" class="headerlink" title="领劵日期 Date_received"></a>领劵日期 Date_received</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">date_received = dfoff[<span class="string">'Date_received'</span>].unique()</span><br><span class="line">date_received = sorted(date_received[date_received != <span class="string">'null'</span>])</span><br><span class="line"></span><br><span class="line">date_buy = dfoff[<span class="string">'Date'</span>].unique()</span><br><span class="line">date_buy = sorted(date_buy[date_buy != <span class="string">'null'</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'优惠卷收到日期从'</span>,date_received[<span class="number">0</span>],<span class="string">'到'</span>,date_received[<span class="number">-1</span>])</span><br><span class="line">print(<span class="string">'消费日期从'</span>,date_buy[<span class="number">0</span>],<span class="string">'到'</span>,date_buy[<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><pre><code>优惠卷收到日期从 20160101 到 20160615消费日期从 20160101 到 20160630</code></pre><p><strong>关于领劵日期的特征：</strong></p><ul><li><p>weekday : {null, 1, 2, 3, 4, 5, 6, 7}</p></li><li><p>weekday_type : {1, 0}（周六和周日为1，其他为0）</p></li><li><p>Weekday_1 : {1, 0, 0, 0, 0, 0, 0}</p></li><li><p>Weekday_2 : {0, 1, 0, 0, 0, 0, 0}</p></li><li><p>Weekday_3 : {0, 0, 1, 0, 0, 0, 0}</p></li><li><p>Weekday_4 : {0, 0, 0, 1, 0, 0, 0}</p></li><li><p>Weekday_5 : {0, 0, 0, 0, 1, 0, 0}</p></li><li><p>Weekday_6 : {0, 0, 0, 0, 0, 1, 0}</p></li><li><p>Weekday_7 : {0, 0, 0, 0, 0, 0, 1}</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getWeekday</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> row == <span class="string">'null'</span>:</span><br><span class="line">        <span class="keyword">return</span> row</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> date(int(row[<span class="number">0</span>:<span class="number">4</span>]), int(row[<span class="number">4</span>:<span class="number">6</span>]), int(row[<span class="number">6</span>:<span class="number">8</span>])).weekday() + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">dfoff[<span class="string">'weekday'</span>] = dfoff[<span class="string">'Date_received'</span>].astype(str).apply(getWeekday)</span><br><span class="line">dftest[<span class="string">'weekday'</span>] = dftest[<span class="string">'Date_received'</span>].astype(str).apply(getWeekday)</span><br><span class="line"></span><br><span class="line"><span class="comment"># weekday_type :  周六和周日为1，其他为0</span></span><br><span class="line">dfoff[<span class="string">'weekday_type'</span>] = dfoff[<span class="string">'weekday'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x <span class="keyword">in</span> [<span class="number">6</span>,<span class="number">7</span>] <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">dftest[<span class="string">'weekday_type'</span>] = dftest[<span class="string">'weekday'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x <span class="keyword">in</span> [<span class="number">6</span>,<span class="number">7</span>] <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># change weekday to one-hot encoding </span></span><br><span class="line">weekdaycols = [<span class="string">'weekday_'</span> + str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">8</span>)]</span><br><span class="line"><span class="comment">#print(weekdaycols)</span></span><br><span class="line"></span><br><span class="line">tmpdf = pd.get_dummies(dfoff[<span class="string">'weekday'</span>].replace(<span class="string">'null'</span>, np.nan))</span><br><span class="line">tmpdf.columns = weekdaycols</span><br><span class="line">dfoff[weekdaycols] = tmpdf</span><br><span class="line"></span><br><span class="line">tmpdf = pd.get_dummies(dftest[<span class="string">'weekday'</span>].replace(<span class="string">'null'</span>, np.nan))</span><br><span class="line">tmpdf.columns = weekdaycols</span><br><span class="line">dftest[weekdaycols] = tmpdf</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfoff.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="所有特征："><a href="#所有特征：" class="headerlink" title="所有特征："></a>所有特征：</h3><ul><li><p>discount_rate</p></li><li><p>discount_type</p></li><li><p>discount_man</p></li><li><p>discount_jian</p></li><li><p>distance</p></li><li><p>weekday</p></li><li><p>weekday_type</p></li><li><p>weekday_1</p></li><li><p>weekday_2</p></li><li><p>weekday_3</p></li><li><p>weekday_4</p></li><li><p>weekday_5</p></li><li><p>weekday_6</p></li><li><p>weekday_7</p></li></ul><h2 id="标签标注"><a href="#标签标注" class="headerlink" title="标签标注"></a>标签标注</h2><p>三种情况：</p><ul><li><p>Date_received == ‘null’：表示没有领到优惠券，无需考虑，y = -1</p></li><li><p>(Date_received != ‘null’) &amp; (Date != ‘null’) &amp; (Date - Date_received &lt;= 15)：表示领取优惠券且在15天内使用，即正样本，y = 1</p></li><li><p>(Date_received != ‘null’) &amp; ((Date == ‘null’) | (Date - Date_received &gt; 15))：表示领取优惠券未在在15天内使用，即负样本，y = 0</p></li></ul><p>定义标签备注函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">label</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> row[<span class="string">'Date_received'</span>] == <span class="string">'null'</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">if</span> row[<span class="string">'Date'</span>] != <span class="string">'null'</span>:</span><br><span class="line">        td = pd.to_datetime(row[<span class="string">'Date'</span>], format=<span class="string">'%Y%m%d'</span>) - pd.to_datetime(row[<span class="string">'Date_received'</span>], format=<span class="string">'%Y%m%d'</span>)</span><br><span class="line">        <span class="keyword">if</span> td &lt;= pd.Timedelta(<span class="number">15</span>, <span class="string">'D'</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">dfoff[<span class="string">'label'</span>] = dfoff.apply(label, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(dfoff[<span class="string">'label'</span>].value_counts())</span><br></pre></td></tr></table></figure><pre><code> 0    988887-1    701602 1     64395Name: label, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfoff.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h2 id="建立线性模型-SGDClassifier"><a href="#建立线性模型-SGDClassifier" class="headerlink" title="建立线性模型 SGDClassifier"></a>建立线性模型 SGDClassifier</h2><ul><li><p>使用上面提取的14个特征。</p></li><li><p>训练集：20160101-20160515；验证集：20160516-20160615。</p></li><li><p>用线性模型 SGDClassifier</p></li></ul><h3 id="划分训练集-验证集"><a href="#划分训练集-验证集" class="headerlink" title="划分训练集/验证集"></a>划分训练集/验证集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data split</span></span><br><span class="line">df = dfoff[dfoff[<span class="string">'label'</span>] != <span class="number">-1</span>].copy()</span><br><span class="line">train = df[(df[<span class="string">'Date_received'</span>] &lt; <span class="string">'20160516'</span>)].copy()</span><br><span class="line">valid = df[(df[<span class="string">'Date_received'</span>] &gt;= <span class="string">'20160516'</span>) &amp; (df[<span class="string">'Date_received'</span>] &lt;= <span class="string">'20160615'</span>)].copy()</span><br><span class="line">print(<span class="string">'Train Set: \n'</span>, train[<span class="string">'label'</span>].value_counts())</span><br><span class="line">print(<span class="string">'Valid Set: \n'</span>, valid[<span class="string">'label'</span>].value_counts())</span><br></pre></td></tr></table></figure><pre><code>Train Set:  0    7591721     41524Name: label, dtype: int64Valid Set:  0    2297151     22871Name: label, dtype: int64</code></pre><h3 id="特征数量"><a href="#特征数量" class="headerlink" title="特征数量"></a>特征数量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature</span></span><br><span class="line">original_feature = [<span class="string">'discount_rate'</span>,<span class="string">'discount_type'</span>,<span class="string">'discount_man'</span>, <span class="string">'discount_jian'</span>,<span class="string">'distance'</span>, <span class="string">'weekday'</span>, <span class="string">'weekday_type'</span>] + weekdaycols</span><br><span class="line">print(<span class="string">'共有特征：'</span>,len(original_feature),<span class="string">'个'</span>)</span><br><span class="line">print(original_feature)</span><br></pre></td></tr></table></figure><pre><code>共有特征： 14 个[&#39;discount_rate&#39;, &#39;discount_type&#39;, &#39;discount_man&#39;, &#39;discount_jian&#39;, &#39;distance&#39;, &#39;weekday&#39;, &#39;weekday_type&#39;, &#39;weekday_1&#39;, &#39;weekday_2&#39;, &#39;weekday_3&#39;, &#39;weekday_4&#39;, &#39;weekday_5&#39;, &#39;weekday_6&#39;, &#39;weekday_7&#39;]</code></pre><h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_model</span><span class="params">(data, predictors)</span>:</span></span><br><span class="line">    </span><br><span class="line">    classifier = <span class="keyword">lambda</span>: SGDClassifier(</span><br><span class="line">        loss=<span class="string">'log'</span>,  <span class="comment"># loss function: logistic regression</span></span><br><span class="line">        penalty=<span class="string">'elasticnet'</span>, <span class="comment"># L1 &amp; L2</span></span><br><span class="line">        fit_intercept=<span class="literal">True</span>,  <span class="comment"># 是否存在截距，默认存在</span></span><br><span class="line">        max_iter=<span class="number">100</span>, </span><br><span class="line">        shuffle=<span class="literal">True</span>,  <span class="comment"># Whether or not the training data should be shuffled after each epoch</span></span><br><span class="line">        n_jobs=<span class="number">1</span>, <span class="comment"># The number of processors to use</span></span><br><span class="line">        class_weight=<span class="literal">None</span>) <span class="comment"># Weights associated with classes. If not given, all classes are supposed to have weight one.</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 管道机制使得参数集在新数据集（比如测试集）上的重复使用，管道机制实现了对全部步骤的流式化封装和管理。</span></span><br><span class="line">    model = Pipeline(steps=[</span><br><span class="line">        (<span class="string">'ss'</span>, StandardScaler()), <span class="comment"># transformer</span></span><br><span class="line">        (<span class="string">'en'</span>, classifier())  <span class="comment"># estimator</span></span><br><span class="line">    ])</span><br><span class="line"> </span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">'en__alpha'</span>: [ <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>],</span><br><span class="line">        <span class="string">'en__l1_ratio'</span>: [ <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>]</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># StratifiedKFold用法类似Kfold，但是他是分层采样，确保训练集，测试集中各类别样本的比例与原始数据集中相同。</span></span><br><span class="line">    folder = StratifiedKFold(n_splits=<span class="number">3</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Exhaustive search over specified parameter values for an estimator.</span></span><br><span class="line">    grid_search = GridSearchCV(</span><br><span class="line">        model, </span><br><span class="line">        parameters, </span><br><span class="line">        cv=folder, </span><br><span class="line">        n_jobs=<span class="number">-1</span>,  <span class="comment"># -1 means using all processors</span></span><br><span class="line">        verbose=<span class="number">1</span>)</span><br><span class="line">    grid_search = grid_search.fit(data[predictors], </span><br><span class="line">                                  data[<span class="string">'label'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grid_search</span><br></pre></td></tr></table></figure><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictors = original_feature</span><br><span class="line">model = check_model(train, predictors)</span><br></pre></td></tr></table></figure><pre><code>Fitting 3 folds for each of 9 candidates, totalling 27 fits[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:  3.0min finished</code></pre><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>对验证集中每个优惠券预测的结果计算 AUC，再对所有优惠券的 AUC 求平均。计算 AUC 的时候，如果 label 只有一类，就直接跳过，因为 AUC 无法计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># valid predict</span></span><br><span class="line">y_valid_pred = model.predict_proba(valid[predictors])</span><br><span class="line">valid1 = valid.copy()</span><br><span class="line">valid1[<span class="string">'pred_prob'</span>] = y_valid_pred[:, <span class="number">1</span>]</span><br><span class="line">valid1.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>计算 AUC：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># avgAUC calculation</span></span><br><span class="line">vg = valid1.groupby([<span class="string">'Coupon_id'</span>])</span><br><span class="line">aucs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> vg:</span><br><span class="line">    tmpdf = i[<span class="number">1</span>] </span><br><span class="line">    <span class="keyword">if</span> len(tmpdf[<span class="string">'label'</span>].unique()) != <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    fpr, tpr, thresholds = roc_curve(tmpdf[<span class="string">'label'</span>], tmpdf[<span class="string">'pred_prob'</span>], pos_label=<span class="number">1</span>)</span><br><span class="line">    aucs.append(auc(fpr, tpr))</span><br><span class="line">print(np.average(aucs))</span><br></pre></td></tr></table></figure><pre><code>0.532344469452</code></pre><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test prediction for submission</span></span><br><span class="line">y_test_pred = model.predict_proba(dftest[predictors])</span><br><span class="line">dftest1 = dftest[[<span class="string">'User_id'</span>,<span class="string">'Coupon_id'</span>,<span class="string">'Date_received'</span>]].copy()</span><br><span class="line">dftest1[<span class="string">'Probability'</span>] = y_test_pred[:,<span class="number">1</span>]</span><br><span class="line">dftest1.to_csv(<span class="string">'submit1.csv'</span>, index=<span class="literal">False</span>, header=<span class="literal">False</span>)</span><br><span class="line">dftest1.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="保存模型-amp-导入模型"><a href="#保存模型-amp-导入模型" class="headerlink" title="保存模型 &amp; 导入模型"></a>保存模型 &amp; 导入模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(<span class="string">'1_model.pkl'</span>):</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'1_model.pkl'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(model, f)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'1_model.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        model = pickle.load(f)</span><br></pre></td></tr></table></figure><h3 id="改用决策数算法训练模型"><a href="#改用决策数算法训练模型" class="headerlink" title="改用决策数算法训练模型"></a>改用决策数算法训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_model</span><span class="params">(data, predictors)</span>:</span>    </span><br><span class="line">    classifier = DecisionTreeClassifier(random_state=<span class="number">1</span>)    </span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">'max_leaf_nodes'</span>: list(range(<span class="number">2</span>, <span class="number">100</span>)), </span><br><span class="line">        <span class="string">'min_samples_split'</span>: [<span class="number">8</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="comment"># StratifiedKFold用法类似Kfold，但是他是分层采样，确保训练集，测试集中各类别样本的比例与原始数据集中相同。</span></span><br><span class="line">    folder = StratifiedKFold(n_splits=<span class="number">3</span>, shuffle=<span class="literal">True</span>)    </span><br><span class="line">    <span class="comment"># Exhaustive search over specified parameter values for an estimator.</span></span><br><span class="line">    grid_search = GridSearchCV(</span><br><span class="line">        classifier, </span><br><span class="line">        parameters, </span><br><span class="line">        cv=folder, </span><br><span class="line">        n_jobs=<span class="number">-1</span>,  <span class="comment"># -1 means using all processors</span></span><br><span class="line">        verbose=<span class="number">1</span>)</span><br><span class="line">    grid_search = grid_search.fit(data[predictors].values,</span><br><span class="line">    data[<span class="string">'label'</span>].values)<span class="comment">#parallel报错加上了.values</span></span><br><span class="line">    <span class="keyword">return</span> grid_search</span><br><span class="line">    </span><br><span class="line">predictors = original_feature</span><br><span class="line">model = check_model(train, predictors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># valid predict</span></span><br><span class="line">y_valid_pred = model.predict_proba(valid[predictors])</span><br><span class="line">valid1 = valid.copy()</span><br><span class="line">valid1[<span class="string">'pred_prob'</span>] = y_valid_pred[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># avgAUC calculation</span></span><br><span class="line">vg = valid1.groupby([<span class="string">'Coupon_id'</span>])</span><br><span class="line">aucs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> vg:</span><br><span class="line">    tmpdf = i[<span class="number">1</span>] </span><br><span class="line">    <span class="keyword">if</span> len(tmpdf[<span class="string">'label'</span>].unique()) != <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    fpr, tpr, thresholds = roc_curve(tmpdf[<span class="string">'label'</span>], tmpdf[<span class="string">'pred_prob'</span>], pos_label=<span class="number">1</span>)</span><br><span class="line">    aucs.append(auc(fpr, tpr))</span><br><span class="line">print(np.average(aucs))</span><br><span class="line"></span><br><span class="line"><span class="comment"># test prediction for submission</span></span><br><span class="line">y_test_pred = model.predict_proba(dftest[predictors])</span><br><span class="line">dftest1 = dftest[[<span class="string">'User_id'</span>,<span class="string">'Coupon_id'</span>,<span class="string">'Date_received'</span>]].copy()</span><br><span class="line">dftest1[<span class="string">'Probability'</span>] = y_test_pred[:,<span class="number">1</span>]</span><br><span class="line">dftest1.to_csv(<span class="string">'submit1-dt.csv'</span>, index=<span class="literal">False</span>, header=<span class="literal">False</span>)</span><br><span class="line">dftest1.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h2 id="优化模型…"><a href="#优化模型…" class="headerlink" title="优化模型…"></a>优化模型…</h2><ul><li><p><strong>特征工程</strong></p></li><li><p><strong>机器学习算法</strong></p></li><li><p><strong>模型集成</strong></p></li></ul><h2 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h2><p><a href="https://github.com/wepe/O2O-Coupon-Usage-Forecast" target="_blank" rel="noopener">比赛第一名代码与解析</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;赛题链接：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://tianchi.aliyun.com/getStart/introduction.htm?spm=5176.100066.0.0.518433afBqXIKM&amp;amp;raceId
      
    
    </summary>
    
      <category term="天池" scheme="https://17091557073.github.io/categories/%E5%A4%A9%E6%B1%A0/"/>
    
    
      <category term="特征工程" scheme="https://17091557073.github.io/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="交叉验证" scheme="https://17091557073.github.io/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯算法</title>
    <link href="https://17091557073.github.io/2019/05/08/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    <id>https://17091557073.github.io/2019/05/08/朴素贝叶斯算法/</id>
    <published>2019-05-08T08:49:00.000Z</published>
    <updated>2019-06-10T15:33:35.854Z</updated>
    
    <content type="html"><![CDATA[<ol><li>条件概率<br>p(c|x)=p(x|c)*p(c)/p(x)<br><a href="https://mp.weixin.qq.com/s/7xRyZJpXmeB77MZNLqVf3w" target="_blank" rel="noopener">联合概率和全概率公式</a></li><li>朴素贝叶斯<br>朴素：假设每个特征之间<strong>相互独立</strong>且<strong>同等重要</strong>；条件概率公式中的联合概率p(x0,x1…xN|c)等价于p(x0|c)<em>p(x1|c)</em>…p(xN|c),极大地简化了计算过程；主要有<a href="https://blog.csdn.net/lming_08/article/details/37542331" target="_blank" rel="noopener">两种实现方式</a>：<ul><li><strong>贝努利模型（词集模型）：</strong>考虑词在文档中出现与否（相当于特征是等权重的）；P(c)= 类c下文件总数/整个训练样本的文档总数<br>P(tk|c)=(类c下包括单词tk的文件数+1)/(类c下文档总数+2) ；  在这里，m=2, p=1/2。  这里一定要注意：伯努利是以文档为粒度的，所以分母是文档总数，而不是网上以讹传讹的类c下单词总数</li><li><strong>多项式模型（词袋模型）：</strong>考虑词在文档中出现的次数<br>在多项式模型中。 设某文档d=(t1,t2,…,tk)。tk是该文档中出现过的单词。同意反复。则<br>先验概率P(c)= 类c下单词总数/整个训练样本的单词总数<br>类条件概率P(tk|c)=(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|)<br>V是训练样本的单词表（即抽取单词集合。单词出现多次，仅仅算一个），|V|则表示训练样本包括多少种单词。在这里，m=|V|, p=1/|V|。<br>P(tk|c)能够看作是单词tk在证明d属于类c上提供了多大的证据。而P(c)则能够觉得是类别c在总体上占多大比例(有多大可能性)。</li></ul></li><li>文档分类<br>先分词，把每个词出现或不出现作为一个特征；特征数量较大，故采用直方图来分析数据效果较好；要得到好的概率分布，需要足够的数据样本，特征数越多，所需样本量越大（假设每个特征需要N个样本，10个特征则需要N^10数据量，如果相互独立，数量可降至N*10）；最后计算每个独立特征的条件概率;<br>在文档分类问题中，词袋模型比词集模型效果更好些；除此之外，还可以通过移除高频词、停用词，优化单词切分器来提高准确率。</li></ol><hr><h6 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Thu May  9 14:14:53 2019</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: LiGuan</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@desc: 朴素贝叶斯算法</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    生成训练数据#文档数据来自斑点狗爱好者留言板</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    postingList=[[<span class="string">'my'</span>,<span class="string">'dog'</span>,<span class="string">'has'</span>,<span class="string">'flea'</span>,<span class="string">'problem'</span>,<span class="string">'help'</span>,<span class="string">'please'</span>],</span><br><span class="line">                 [<span class="string">'maybe'</span>,<span class="string">'not'</span>,<span class="string">'take'</span>,<span class="string">'him'</span>,<span class="string">'to'</span>,<span class="string">'dog'</span>,<span class="string">'park'</span>,<span class="string">'stupid'</span>],</span><br><span class="line">                 [<span class="string">'my'</span>,<span class="string">'dalmations'</span>,<span class="string">'is'</span>,<span class="string">'so'</span>,<span class="string">'cute'</span>,<span class="string">'I'</span>,<span class="string">'love'</span>,<span class="string">'him'</span>],</span><br><span class="line">                 [<span class="string">'stop'</span>,<span class="string">'posting'</span>,<span class="string">'stupid'</span>,<span class="string">'worthless'</span>,<span class="string">'garbage'</span>],</span><br><span class="line">                 [<span class="string">'mr'</span>,<span class="string">'licks'</span>,<span class="string">'ate'</span>,<span class="string">'my'</span>,<span class="string">'steak'</span>,<span class="string">'how'</span>,<span class="string">'to'</span>,<span class="string">'stop'</span>,<span class="string">'him'</span>],</span><br><span class="line">                 [<span class="string">'quit'</span>,<span class="string">'buying'</span>,<span class="string">'worthless'</span>,<span class="string">'dog'</span>,<span class="string">'food'</span>,<span class="string">'stupid'</span>]]<span class="comment">#(),[],&#123;&#125;中的换行不用加 \</span></span><br><span class="line">    classVec=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]<span class="comment">#1代表侮辱性文字，0代表正常言论；文本类别由人工标注</span></span><br><span class="line">    <span class="keyword">return</span> postingList,classVec</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    创建一个文档中不重复词的列表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    vocabSet=set([])<span class="comment">#创建一个空集</span></span><br><span class="line">    <span class="keyword">for</span> documnet <span class="keyword">in</span> dataSet:</span><br><span class="line">        vocabSet=vocabSet | set(documnet)<span class="comment">#创建两个集合的并集</span></span><br><span class="line">    <span class="keyword">return</span> list(vocabSet)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span><span class="params">(vocabList,inputSet)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    将一篇分词后文档转为词向量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    returnVec=[<span class="number">0</span>]*len(vocabList)<span class="comment">#创建一个元素都为0的向量</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)]=<span class="number">1</span><span class="comment">#词汇表中的单词在输入文档中出现与否（0|1）</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'the word: %s is not in my Vocabulary!'</span>% word)</span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix,trainCategory)</span>:</span></span><br><span class="line">    numTrainDocs=len(trainMatrix)</span><br><span class="line">    numWords=len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    pAbusive=sum(trainCategory)/numTrainDocs</span><br><span class="line">    p0Num,p1Num=np.ones(numWords),np.ones(numWords)</span><br><span class="line">    <span class="comment">#拉普拉斯平滑，解决零概率的问题</span></span><br><span class="line">    p0Denom,p1Denom=<span class="number">2</span>,<span class="number">2</span><span class="comment">#将初始化分子由0改成1，分母由0改成2，为了防止其中某一项为0时整个值变成0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i]==<span class="number">1</span>:</span><br><span class="line">            p1Num+=trainMatrix[i]</span><br><span class="line">            p1Denom+=<span class="number">1</span></span><br><span class="line">            <span class="comment">#p1Denom+=sum(trainMatrix[i])</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num+=trainMatrix[i]</span><br><span class="line">            p0Denom+=<span class="number">1</span></span><br><span class="line">            <span class="comment">#p0Denom+=sum(trainMatrix[i])</span></span><br><span class="line">    p1Vect=np.log(p1Num/p1Denom)</span><br><span class="line">    p0Vect=np.log(p0Num/p0Denom)<span class="comment">#改成取对数是为了避免数值下溢出（大部分因子都特别小，导致总乘积接近0）</span></span><br><span class="line">    <span class="keyword">return</span> p0Vect,p1Vect,pAbusive</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify,p0Vec,p1Vec,pClass1)</span>:</span></span><br><span class="line">    <span class="comment">#对应元素相乘。logA * B = logA + logB</span></span><br><span class="line">    p1=np.sum(vec2Classify*p1Vec)+np.log(pClass1)</span><br><span class="line">    p0=np.sum(vec2Classify*p0Vec)+np.log(<span class="number">1.0</span>-pClass1)</span><br><span class="line">    <span class="keyword">if</span> p1&gt;p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    封装了生成数据，调用算法的所有操作</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    listOPosts,listClasses=loadDataSet()</span><br><span class="line">    myVocabList=createVocabList(listOPosts)</span><br><span class="line">    trainMat=[]</span><br><span class="line">    <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))</span><br><span class="line">    p0V,p1V,pAb=trainNB0(np.array(trainMat),np.array(listClasses))</span><br><span class="line">    testEntry=[<span class="string">'love'</span>,<span class="string">'my'</span>,<span class="string">'dalmation'</span>]</span><br><span class="line">    thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))</span><br><span class="line">    print(testEntry,<span class="string">'classified as:&#123;&#125;'</span>.format(classifyNB(thisDoc,p0V,p1V,pAb)))</span><br><span class="line">    testEntry=[<span class="string">'stupid'</span>,<span class="string">'garbage'</span>]</span><br><span class="line">    thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))</span><br><span class="line">    print(testEntry,<span class="string">'classified as:&#123;&#125;'</span>.format(classifyNB(thisDoc,p0V,p1V,pAb)))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocabList,inputSet)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    词袋模型，单词每遇到一次就加一，区别于词集模型</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    returnVec=[<span class="number">0</span>]*len(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)]+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> returnVec</span><br><span class="line"><span class="comment">#使用朴素贝叶斯进行交叉验证   </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span><span class="params">(bigString)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    listOfTokens=re.split(<span class="string">r'\W+'</span>,bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> len(tok)&gt;<span class="number">2</span>]</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spamTest</span><span class="params">()</span>:</span></span><br><span class="line">    docList,classList,fullText=[],[],[]</span><br><span class="line">    <span class="comment">#导入并解析好坏邮件</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">26</span>):</span><br><span class="line">        wordList=textParse(open(<span class="string">r'C:\Users\ligua\Videos\我的资源\机器学习实战\machinelearninginaction\Ch04\email\spam\%d.txt'</span>%i,encoding=<span class="string">"ISO-8859-1"</span>).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        wordList=textParse(open(<span class="string">r'C:\Users\ligua\Videos\我的资源\机器学习实战\machinelearninginaction\Ch04\email\ham\%d.txt'</span>%i,encoding=<span class="string">"ISO-8859-1"</span>).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    vocabList=createVocabList(docList)</span><br><span class="line">    trainingSet=list(range(<span class="number">50</span>));testSet=[]</span><br><span class="line">    <span class="comment">#随机构建训练集</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        randIndex=int(random.uniform(<span class="number">0</span>,len(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat,trainClasses=[],[]</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V,p1V,pSpam=trainNB0(np.array(trainMat),np.array(trainClasses))</span><br><span class="line">    errorCount=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        <span class="comment">#对测试集进行分类</span></span><br><span class="line">        wordVector=setOfWords2Vec(vocabList,docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(np.array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:</span><br><span class="line">            errorCount+=<span class="number">1</span></span><br><span class="line">            print(<span class="string">"classification error"</span>, docList[docIndex])</span><br><span class="line">    print(<span class="string">'the error rate is:%f'</span>%(float(errorCount)/len(testSet)))</span><br></pre></td></tr></table></figure><h5 id="模块调用"><a href="#模块调用" class="headerlink" title="模块调用"></a>模块调用</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Thu May  9 15:21:05 2019</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: LiGuan</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@desc: </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> bayes</span><br><span class="line"><span class="keyword">import</span> imp</span><br><span class="line"></span><br><span class="line">imp.reload(bayes)</span><br><span class="line">listOPosts,listClasses=bayes.loadDataSet()</span><br><span class="line">myVocabList=bayes.createVocabList(listOPosts)</span><br><span class="line">bayes.setOfWords2Vec(myVocabList,listOPosts[<span class="number">0</span>])</span><br><span class="line">bayes.setOfWords2Vec(myVocabList,listOPosts[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">imp.reload(bayes)</span><br><span class="line">myVocabList=bayes.createVocabList(listOPosts)</span><br><span class="line">trainMat=[]</span><br><span class="line"><span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">    trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))</span><br><span class="line">p0V,p1V,pAb=bayes.trainNB0(trainMat,listClasses)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imp.reload(bayes)</span><br><span class="line">listOPosts,listClasses=bayes.loadDataSet()</span><br><span class="line">myVocabList=bayes.createVocabList(listOPosts)</span><br><span class="line">trainMat=[]</span><br><span class="line"><span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">    trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))</span><br><span class="line"><span class="comment">#输出两个类别的概率向量（条件概率），和属于侮辱性文档的概率(先验概率)</span></span><br><span class="line">p0V,p1V,pAb=bayes.trainNB0(trainMat,listClasses)</span><br><span class="line"></span><br><span class="line">imp.reload(bayes)</span><br><span class="line">bayes.testingNB()</span><br><span class="line"></span><br><span class="line">mySent=<span class="string">'This book is the best book on Python or M.L. I have ever laid eyes upon'</span></span><br><span class="line">mySent.split()</span><br><span class="line"><span class="comment">#引入正则表达式，将除单词、数字外的任意字符作为分隔符</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">regEx=re.compile(<span class="string">'\\W*'</span>)</span><br><span class="line">listOfTokens=regEx.split(mySent)</span><br><span class="line">emailText=open(<span class="string">r'C:\Users\ligua\Videos\我的资源\机器学习实战\machinelearninginaction\Ch04\email\ham\6.txt'</span>).read()</span><br><span class="line">listOfTokens=regEx.split(emailText)</span><br><span class="line"></span><br><span class="line">imp.reload(bayes)</span><br><span class="line">bayes.spamTest()<span class="comment">#交叉验证，重复执行该方法10次，求平均错误率</span></span><br></pre></td></tr></table></figure><hr><h5 id="朴素贝叶斯算法应用"><a href="#朴素贝叶斯算法应用" class="headerlink" title="朴素贝叶斯算法应用"></a>朴素贝叶斯算法应用</h5><p><img src="/2019/05/08/朴素贝叶斯算法/朴素贝叶斯算法应用.jpg" alt="朴素贝叶斯算法应用"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;条件概率&lt;br&gt;p(c|x)=p(x|c)*p(c)/p(x)&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/7xRyZJpXmeB77MZNLqVf3w&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;联合概率和
      
    
    </summary>
    
      <category term="机器学习实战" scheme="https://17091557073.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="条件概率" scheme="https://17091557073.github.io/tags/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87/"/>
    
      <category term="垃圾邮件" scheme="https://17091557073.github.io/tags/%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>决策树算法</title>
    <link href="https://17091557073.github.io/2019/05/04/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://17091557073.github.io/2019/05/04/决策树/</id>
    <published>2019-05-04T14:38:00.000Z</published>
    <updated>2019-06-10T15:34:31.978Z</updated>
    
    <content type="html"><![CDATA[<h6 id="step1"><a href="#step1" class="headerlink" title="step1:"></a>step1:</h6><p>依据哪个属性来划分数据，使无序数据变得更加有序；划分数据集前后信息量发生的变化称为信息增益，计算每个特征属性划分数据集时的信息增益，最高那个特征就是最好的选择</p><h6 id="step2"><a href="#step2" class="headerlink" title="step2:"></a>step2:</h6><p>以此递归直至遍历完所有特征属性，或者每个分支下所有实例具有相同的分类（存在某些算法C4.5,CART划分时不消耗特征，以后再讨论），ID3算法只能处理离散性特征</p><h6 id="step3"><a href="#step3" class="headerlink" title="step3:"></a>step3:</h6><p>如果已经处理了所有的特征，但类标签还不是唯一的，通常我们会采用多数表决法来确定叶子节点的分类</p><hr><h6 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算给定数据集的香农熵</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    numEntries=len(dataSet)</span><br><span class="line">    labelCounts=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        currentLabel=featVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel]=<span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel]+=<span class="number">1</span></span><br><span class="line">    shannonEnt=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob=float(labelCounts[key])/numEntries</span><br><span class="line">        shannonEnt-=prob*log(prob,<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataSet=[[<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],</span><br><span class="line">             [<span class="number">1</span>,<span class="number">0</span>,<span class="string">'no'</span>],</span><br><span class="line">             [<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>],</span><br><span class="line">             [<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>]]</span><br><span class="line">    labels=[<span class="string">'no surfacing'</span>,<span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet,labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet,axis,value)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">     按照给定特征划分数据集</span></span><br><span class="line"><span class="string">     '''</span></span><br><span class="line">    retDataSet=[]<span class="comment">#函数中传递的是列表的引用，函数中修改会影响整个生存周期,所以在每次调用的时候都新建一个</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis]==value:</span><br><span class="line">            reducedFeatVec=featVec[:axis]</span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    选择最好的数据集划分方式（按哪个特征划分信息增益最大）</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    numFeatures=len(dataSet[<span class="number">0</span>])<span class="number">-1</span></span><br><span class="line">    baseEntropy=calcShannonEnt(dataSet)</span><br><span class="line">    bestInfoGain=<span class="number">0.0</span></span><br><span class="line">    bestFeature=<span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">        featList=[example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        uniqueVals=set(featList)</span><br><span class="line">        newEntropy=<span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet=splitDataSet(dataSet,i,value)</span><br><span class="line">            prob=len(subDataSet)/float(len(dataSet))</span><br><span class="line">            newEntropy+=prob*calcShannonEnt(subDataSet)</span><br><span class="line">        infoGain=baseEntropy-newEntropy</span><br><span class="line">        <span class="keyword">if</span> infoGain&gt;bestInfoGain:</span><br><span class="line">            bestInfoGain=infoGain</span><br><span class="line">            bestFeature=i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    多数表决法确定叶子节点分类</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys():</span><br><span class="line">            classCount[vote]=<span class="number">0</span></span><br><span class="line">        classCount[vote]+=<span class="number">1</span></span><br><span class="line">    sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    递归函数实现整棵树</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    classList=[example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>])==len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]<span class="comment">#类别完全相同则停止继续划分</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>])==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)<span class="comment">#遍历完所有特征时返回出现次数最多的</span></span><br><span class="line">    bestFeat=chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel=labels[bestFeat]</span><br><span class="line">    myTree=&#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    featValues=[example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals=set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels=labels[:]</span><br><span class="line">        myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    字典决策树的使用</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    firstStr=list(inputTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict=inputTree[firstStr]</span><br><span class="line">    featIndex=featLabels.index(firstStr)</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> testVec[featIndex]==key:</span><br><span class="line">            <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:</span><br><span class="line">                classLabel=classify(secondDict[key],featLabels,testVec)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                classLabel=secondDict[key]</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="comment">#决策树字典的序列化化和反序列化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree,filename)</span>:</span></span><br><span class="line">    fw=open(filename,<span class="string">'w'</span>)</span><br><span class="line">    pickle.dump(inputTree,fw)</span><br><span class="line">    fw.close()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr=open(filename)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><h6 id="模块调用"><a href="#模块调用" class="headerlink" title="模块调用"></a>模块调用</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> trees</span><br><span class="line"></span><br><span class="line">myDat,labels=trees.createDataSet()</span><br><span class="line">trees.calcShannonEnt(myDat)<span class="comment">#0.97095</span></span><br><span class="line"></span><br><span class="line">myDat[<span class="number">0</span>][<span class="number">-1</span>]=<span class="string">'maybe'</span><span class="comment">#添加分类</span></span><br><span class="line">trees.calcShannonEnt(myDat)<span class="comment">#1.37095</span></span><br><span class="line"><span class="comment">#数据越混乱，熵值越高</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imp</span><br><span class="line">imp.reload(trees)</span><br><span class="line">myDat,labels=trees.createDataSet()</span><br><span class="line">trees.splitDataSet(myDat,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">imp.reload(trees)</span><br><span class="line">trees.chooseBestFeatureToSplit(myDat)</span><br><span class="line"></span><br><span class="line">imp.reload(trees)</span><br><span class="line">myDat,labels=trees.createDataSet()</span><br><span class="line">mytree=trees.createTree(myDat,labels)</span><br><span class="line"><span class="comment">#&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;</span></span><br><span class="line"></span><br><span class="line">imp.reload(trees)</span><br><span class="line">trees.classify(mytree,labels,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">trees.classify(mytree,labels,[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">imp.reload(trees)</span><br><span class="line">fr=open(<span class="string">'.\machinelearninginaction\Ch03\lenses.txt'</span>)</span><br><span class="line">lenses=[inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]<span class="comment">#跟游标类似，只能读取一次</span></span><br><span class="line">lensesLabels=[<span class="string">'age'</span>,<span class="string">'prescript'</span>,<span class="string">'astigmatic'</span>,<span class="string">'tearRate'</span>]</span><br><span class="line">lensesTree=trees.createTree(lenses,lensesLabels)</span><br></pre></td></tr></table></figure><hr><h6 id="手动计算信息增益"><a href="#手动计算信息增益" class="headerlink" title="手动计算信息增益"></a>手动计算信息增益</h6><p><img src="/2019/05/04/决策树/信息增益.jpg" alt="信息增益计算"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h6 id=&quot;step1&quot;&gt;&lt;a href=&quot;#step1&quot; class=&quot;headerlink&quot; title=&quot;step1:&quot;&gt;&lt;/a&gt;step1:&lt;/h6&gt;&lt;p&gt;依据哪个属性来划分数据，使无序数据变得更加有序；划分数据集前后信息量发生的变化称为信息增益，计算每个特征属性划分
      
    
    </summary>
    
      <category term="机器学习实战" scheme="https://17091557073.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="ID3" scheme="https://17091557073.github.io/tags/ID3/"/>
    
      <category term="信息增益" scheme="https://17091557073.github.io/tags/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A/"/>
    
  </entry>
  
  <entry>
    <title>k-近邻算法</title>
    <link href="https://17091557073.github.io/2019/05/02/KNN/"/>
    <id>https://17091557073.github.io/2019/05/02/KNN/</id>
    <published>2019-05-02T07:33:00.000Z</published>
    <updated>2019-06-10T15:36:02.748Z</updated>
    
    <content type="html"><![CDATA[<h4 id="按照距离最近的K个样本的标签来决定未知样本的标签结果"><a href="#按照距离最近的K个样本的标签来决定未知样本的标签结果" class="headerlink" title="按照距离最近的K个样本的标签来决定未知样本的标签结果"></a>按照距离最近的K个样本的标签来决定未知样本的标签结果</h4><ol><li>k 值的确定：可以通过选择不同的 k 值比较分类效果来确定最佳 k 值（先大范围初筛，再小范围细筛）</li><li>基于距离比较，所以样本各特征之间的取值范围差别较大的时候，应该对特征进行归一化处理，提升分类效果（要消除量纲对特征的影响）</li><li>没有模型的训练过程，每进来一个新样本，会与每个旧样本进行距离的计算，时间及空间复杂度较大，故不适用于大数据量（需要存储空间和计算时间）</li><li>k-近邻算法是基于实例的学习，使用算法时须保证存在接近实际数据的训练样本数据</li></ol><h4 id="python代码实现（含约会网站配对和手写数字识别）"><a href="#python代码实现（含约会网站配对和手写数字识别）" class="headerlink" title="python代码实现（含约会网站配对和手写数字识别）"></a>python代码实现（含约会网站配对和手写数字识别）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed May  1 09:30:09 2019</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: liguan</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@desc: KNN</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">  group=np.array([[<span class="number">1.0</span>,<span class="number">1.1</span>],</span><br><span class="line">                  [<span class="number">1.0</span>,<span class="number">1.0</span>],</span><br><span class="line">                  [<span class="number">0.0</span>,<span class="number">0.0</span>],</span><br><span class="line">                  [<span class="number">0.0</span>,<span class="number">0.1</span>]])</span><br><span class="line">  labels=[<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>]</span><br><span class="line">  <span class="keyword">return</span> group,labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX,dataSet,labels,k)</span>:</span><span class="comment">#inX需要分类的测试集，dataSet已有的训练集,labels标签</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  简易版KNN实现</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  <span class="comment">#计算距离</span></span><br><span class="line">  dataSetSize=dataSet.shape[<span class="number">0</span>]</span><br><span class="line">  diffMat=np.tile(inX,(dataSetSize,<span class="number">1</span>))-dataSet<span class="comment">#将矩阵纵向粘贴复制</span></span><br><span class="line">  sqDiffMat=diffMat**<span class="number">2</span></span><br><span class="line">  sqDistances=sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">  distances=sqDistances**<span class="number">0.5</span></span><br><span class="line">  <span class="comment">#选择距离最小的k个点</span></span><br><span class="line">  sortedDistIndicies=distances.argsort()<span class="comment">#返回的是数组值从小到大的索引值</span></span><br><span class="line">  classCount=&#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    voteIlabel=labels[sortedDistIndicies[i]]</span><br><span class="line">    classCount[voteIlabel]=classCount.get(voteIlabel,<span class="number">0</span>)+<span class="number">1</span><span class="comment">#如果指定键不存在的话，返回0</span></span><br><span class="line">  sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)<span class="comment">#根据第一个域进行排序</span></span><br><span class="line">  <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span><span class="params">(filename)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  csv文件转成属性标签矩阵</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  fr=open(filename)</span><br><span class="line">  arrayOLines=fr.readlines()</span><br><span class="line">  fr.close()</span><br><span class="line">  numberOfLines=len(arrayOLines)</span><br><span class="line">  returnMat=np.zeros((numberOfLines,<span class="number">3</span>))</span><br><span class="line">  classLabelVector=[]</span><br><span class="line">  index=<span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> line <span class="keyword">in</span> arrayOLines:</span><br><span class="line">    line=line.strip()</span><br><span class="line">    listFromLine=line.split(<span class="string">'\t'</span>)</span><br><span class="line">    returnMat[index,:]=listFromLine[<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">    classLabelVector.append(int(listFromLine[<span class="number">-1</span>]))</span><br><span class="line">    index+=<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> returnMat,classLabelVector</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  归一化特征</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  minVals=dataSet.min(<span class="number">0</span>)<span class="comment">#从列中选取最小值</span></span><br><span class="line">  maxVals=dataSet.max(<span class="number">0</span>)</span><br><span class="line">  ranges=maxVals-minVals</span><br><span class="line">  normDataSet=np.zeros(np.shape(dataSet))</span><br><span class="line">  m=dataSet.shape[<span class="number">0</span>]</span><br><span class="line">  normDataSet=dataSet-np.tile(minVals,(m,<span class="number">1</span>))</span><br><span class="line">  normDataSet/=np.tile(ranges,(m,<span class="number">1</span>))</span><br><span class="line">  <span class="keyword">return</span> normDataSet,ranges,minVals</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">  hoRatio=<span class="number">0.10</span><span class="comment">#设置测试集比率</span></span><br><span class="line">  datingDataMat,datingLabels=file2matrix(<span class="string">'.\machinelearninginaction\Ch02\datingTestSet2.txt'</span>)</span><br><span class="line">  normMat,ranges,minVals=autoNorm(datingDataMat)</span><br><span class="line">  m=normMat.shape[<span class="number">0</span>]</span><br><span class="line">  numTestVecs=int(m*hoRatio)</span><br><span class="line">  errorCount=<span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs):</span><br><span class="line">    classifierResult=classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],<span class="number">3</span>)</span><br><span class="line">    print(<span class="string">'分类器返回：%d;真实的结果：%d'</span>%(classifierResult,datingLabels[i]))</span><br><span class="line">    <span class="keyword">if</span> classifierResult != datingLabels[i]:</span><br><span class="line">      errorCount+=<span class="number">1</span></span><br><span class="line">  print(<span class="string">'总错误率：%f'</span>%(errorCount/numTestVecs))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  将一个32*32的二进制图像矩阵转换成1*1024的向量</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  returnVect=np.zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">  fr=open(filename)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">    lineStr=fr.readline()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">      returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j]=int(lineStr[j])</span><br><span class="line">  <span class="keyword">return</span> returnVect</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwriteClassTest</span><span class="params">()</span>:</span></span><br><span class="line">  hwLabels=[]</span><br><span class="line">  trainingFileList=os.listdir(<span class="string">'./machinelearninginaction/Ch02/trainingDigits'</span>)</span><br><span class="line">  m=len(trainingFileList)</span><br><span class="line">  trainingMat=np.zeros((m,<span class="number">1024</span>))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    fileNameStr=trainingFileList[i]</span><br><span class="line">    fileStr=fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">    classNumStr=int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">    hwLabels.append(classNumStr)</span><br><span class="line">    trainingMat[i,:]=img2vector(<span class="string">'./machinelearninginaction/Ch02/trainingDigits/%s'</span>%fileNameStr)</span><br><span class="line">  testFileList=os.listdir(<span class="string">'./machinelearninginaction/Ch02/testDigits'</span>)</span><br><span class="line">  errorCount=<span class="number">0.0</span></span><br><span class="line">  mTest=len(testFileList)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):</span><br><span class="line">    fileNameStr=testFileList[i]</span><br><span class="line">    fileStr=fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">    classNumStr=int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">    vectorUnderTest=img2vector(<span class="string">'./machinelearninginaction/Ch02/testDigits/%s'</span>%fileNameStr)</span><br><span class="line">    classifierResult=classify0(vectorUnderTest,trainingMat,hwLabels,<span class="number">3</span>)</span><br><span class="line">    print(<span class="string">'分类器返回：%d;真实的结果：%d'</span>%(classifierResult,classNumStr))</span><br><span class="line">    <span class="keyword">if</span> classifierResult!=classNumStr:</span><br><span class="line">      errorCount+=<span class="number">1.0</span></span><br><span class="line">  print(<span class="string">'错误总数：%d'</span>%errorCount)</span><br><span class="line">  print(<span class="string">'总错误率：%f'</span>%(errorCount/mTest))</span><br></pre></td></tr></table></figure><h4 id="模块代码调用"><a href="#模块代码调用" class="headerlink" title="模块代码调用"></a>模块代码调用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created on Wed May  1 11:09:02 2019</span><br><span class="line"></span><br><span class="line">@author: LiGuan</span><br><span class="line"></span><br><span class="line">@desc: </span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">#导入之前创建的模块</span><br><span class="line">import kNN</span><br><span class="line">#先生成数据</span><br><span class="line">group,labels=kNN.createDataSet()</span><br><span class="line">#再进行预测</span><br><span class="line">kNN.classify0([0,0],group,labels,3)#‘B’</span><br><span class="line"></span><br><span class="line">import imp</span><br><span class="line">imp.reload(kNN)#新增方法，重新加载模块</span><br><span class="line">datingDataMat,datingLabels=kNN.file2matrix(&apos;.\machinelearninginaction\Ch02\datingTestSet2.txt&apos;)</span><br><span class="line"></span><br><span class="line">#制作带样本分类标签的数据散点图</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig=plt.figure()</span><br><span class="line">ax=fig.add_subplot(111)#将画布分割成1行1列，图像画在从左到右从上到下的第1块</span><br><span class="line">ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*np.array(datingLabels),15.0*np.array(datingLabels))#利用颜色和尺寸标识了数据点的属性类别</span><br><span class="line">plt.show()#从结果看，前两个属性更能区分数据点所属类别</span><br><span class="line"></span><br><span class="line">#特征数据归一化</span><br><span class="line">imp.reload(kNN)</span><br><span class="line">normMat,ranges,minVals=kNN.autoNorm(datingDataMat)</span><br><span class="line"></span><br><span class="line">#测试算法</span><br><span class="line">imp.reload(kNN)</span><br><span class="line">kNN.datingClassTest()</span><br><span class="line"></span><br><span class="line">#图像向量转换</span><br><span class="line">imp.reload(kNN)</span><br><span class="line">testVector=kNN.img2vector(&apos;./machinelearninginaction/Ch02/testDigits/0_0.txt&apos;)</span><br><span class="line"></span><br><span class="line">imp.reload(kNN)</span><br><span class="line">kNN.handwriteClassTest()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;按照距离最近的K个样本的标签来决定未知样本的标签结果&quot;&gt;&lt;a href=&quot;#按照距离最近的K个样本的标签来决定未知样本的标签结果&quot; class=&quot;headerlink&quot; title=&quot;按照距离最近的K个样本的标签来决定未知样本的标签结果&quot;&gt;&lt;/a&gt;按照距离最近的K
      
    
    </summary>
    
      <category term="机器学习实战" scheme="https://17091557073.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="约会网站配对" scheme="https://17091557073.github.io/tags/%E7%BA%A6%E4%BC%9A%E7%BD%91%E7%AB%99%E9%85%8D%E5%AF%B9/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://17091557073.github.io/2019/05/01/hello-world/"/>
    <id>https://17091557073.github.io/2019/05/01/hello-world/</id>
    <published>2019-05-01T06:31:00.000Z</published>
    <updated>2019-06-10T15:36:54.145Z</updated>
    
    <content type="html"><![CDATA[<p>工作快四年，回想一下，自己总是很难完整得看完一本书；哪怕看完了，也是属于那种完成任务式的自欺欺人，合上最后一页，甚至总结不出这本书讲了什么，细思极恐，究其原因，觉得还是得要做些笔记；</p><p>一直很想写博客，也尝试过传统的CSDN、博客园，到后来却都没坚持下去，不了了之；看到现在很多人开始自己建网站写博客，感觉挺新奇酷炫，一颗好奇烦躁的心又开始燃烧了起来；</p><p>记一些工作生活中自己的所感所得，也算是对自己的一个阶段性总结吧，希望这次能坚持下来，录下自己的一个学习成长过程！！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;工作快四年，回想一下，自己总是很难完整得看完一本书；哪怕看完了，也是属于那种完成任务式的自欺欺人，合上最后一页，甚至总结不出这本书讲了什么，细思极恐，究其原因，觉得还是得要做些笔记；&lt;/p&gt;
&lt;p&gt;一直很想写博客，也尝试过传统的CSDN、博客园，到后来却都没坚持下去，不了了
      
    
    </summary>
    
      <category term="Daily life" scheme="https://17091557073.github.io/categories/Daily-life/"/>
    
    
  </entry>
  
</feed>
